{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(qkv, bsz, q_len, group_size, num_heads, head_dim):\n",
    "    qkv[:, num_heads // 2:] = qkv[:, num_heads // 2:].roll(-group_size // 2, dims=2)\n",
    "    qkv = qkv.transpose(1, 2).reshape(bsz * (q_len // group_size), group_size, num_heads, head_dim).transpose(1, 2)\n",
    "    return qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [3,5,1,1,1,2,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nums.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "letterMap = [\n",
    "            \"\",     # 0\n",
    "            \"\",     # 1\n",
    "            \"abc\",  # 2\n",
    "            \"def\",  # 3\n",
    "            \"ghi\",  # 4\n",
    "            \"jkl\",  # 5\n",
    "            \"mno\",  # 6\n",
    "            \"pqrs\", # 7\n",
    "            \"tuv\",  # 8\n",
    "            \"wxyz\"  # 9\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letterMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dic = {}\n",
    "\n",
    "for i in range(len(nums)):\n",
    "    dic[nums[i]] = dic.get(nums[i],0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.get(nums[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3, 2: 2, 3: 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(eval(f'1+2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyQueue:\n",
    "    def __init__(self):\n",
    "        self.que_in = deque()\n",
    "        self.que_out = deque()\n",
    "\n",
    "    def push(self, x: int) -> None:\n",
    "        self.que_in.append(x)\n",
    "    \n",
    "    def pop(self) -> int:\n",
    "        if self.empty():\n",
    "            return None\n",
    "        for i in range(len(self.que_in)-1):\n",
    "            self.que_out.append(self.que_in.popleft())\n",
    "        self.que_in, self.que_out = self.que_out, self.que_in\n",
    "        return self.que_out.popleft()\n",
    "\n",
    "\n",
    "    def top(self) -> int:\n",
    "        if self.empty():\n",
    "            return None\n",
    "        ans = self.que_in[-1]\n",
    "        return ans\n",
    "\n",
    "\n",
    "    def empty(self) -> bool:\n",
    "        return not(self.que_in or self.que_out)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2007248401641846\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # 一个简单的线性层\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 实例化模型和损失函数\n",
    "model = SimpleModel()\n",
    "criterion = nn.MarginRankingLoss(margin=1.0)  # 设置边界值为1.0\n",
    "\n",
    "# 假设我们有两个输入x1和x2，以及它们的正确排序关系y\n",
    "x1 = torch.tensor([[1.0]])\n",
    "x2 = torch.tensor([[2.0]])\n",
    "y = torch.tensor([[-1.0]])  # 我们期望x2大于x1\n",
    "\n",
    "# 计算模型的输出\n",
    "output1 = model(x1)\n",
    "output2 = model(x2)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(output1, output2, y)\n",
    "\n",
    "# 打印损失值\n",
    "print(loss.item())\n",
    "\n",
    "# 反向传播和优化\n",
    "loss.backward()\n",
    "# 假设我们有一个优化器\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1863]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3870]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2007, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(output1, output2,  y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llamaconfig = LlamaConfig(vocab_size=32000,\n",
    "                            hidden_size=4096//2,\n",
    "                            intermediate_size=11008//2,\n",
    "                            num_hidden_layers=32//2,\n",
    "                            num_attention_heads=32//2,\n",
    "                            max_position_embeddings=2048//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.llama import  LlamaModel,LlamaConfig, tokenization_llama\n",
    "inputs_ids = torch.randint(\n",
    "    low=0, high=llamaconfig.vocab_size, size=(4,30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25335, 31512,  5938,  1857, 21167, 17652, 25922,  6420, 16560,  6122,\n",
       "        23392, 31341, 30285,   962, 14008, 11489, 19566, 13336,  5855, 19045,\n",
       "        26468, 15024, 27491,  5106, 21616, 23265,  1594, 26583, 13288, 16050])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "bsz = 2\n",
    "num_heads = 4\n",
    "q_len =6\n",
    "head_dim = 3\n",
    "group_size = 2\n",
    "\n",
    "qkv = torch.arange(bsz * num_heads * q_len * head_dim).reshape(bsz, num_heads, q_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6, 3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv[:, num_heads // 2:] = qkv[:, num_heads // 2:].roll(-4 // 2, dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0,   1,   2],\n",
       "          [  3,   4,   5],\n",
       "          [  6,   7,   8],\n",
       "          [  9,  10,  11],\n",
       "          [ 12,  13,  14],\n",
       "          [ 15,  16,  17]],\n",
       "\n",
       "         [[ 18,  19,  20],\n",
       "          [ 21,  22,  23],\n",
       "          [ 24,  25,  26],\n",
       "          [ 27,  28,  29],\n",
       "          [ 30,  31,  32],\n",
       "          [ 33,  34,  35]],\n",
       "\n",
       "         [[ 42,  43,  44],\n",
       "          [ 45,  46,  47],\n",
       "          [ 48,  49,  50],\n",
       "          [ 51,  52,  53],\n",
       "          [ 36,  37,  38],\n",
       "          [ 39,  40,  41]],\n",
       "\n",
       "         [[ 60,  61,  62],\n",
       "          [ 63,  64,  65],\n",
       "          [ 66,  67,  68],\n",
       "          [ 69,  70,  71],\n",
       "          [ 54,  55,  56],\n",
       "          [ 57,  58,  59]]],\n",
       "\n",
       "\n",
       "        [[[ 72,  73,  74],\n",
       "          [ 75,  76,  77],\n",
       "          [ 78,  79,  80],\n",
       "          [ 81,  82,  83],\n",
       "          [ 84,  85,  86],\n",
       "          [ 87,  88,  89]],\n",
       "\n",
       "         [[ 90,  91,  92],\n",
       "          [ 93,  94,  95],\n",
       "          [ 96,  97,  98],\n",
       "          [ 99, 100, 101],\n",
       "          [102, 103, 104],\n",
       "          [105, 106, 107]],\n",
       "\n",
       "         [[114, 115, 116],\n",
       "          [117, 118, 119],\n",
       "          [120, 121, 122],\n",
       "          [123, 124, 125],\n",
       "          [108, 109, 110],\n",
       "          [111, 112, 113]],\n",
       "\n",
       "         [[132, 133, 134],\n",
       "          [135, 136, 137],\n",
       "          [138, 139, 140],\n",
       "          [141, 142, 143],\n",
       "          [126, 127, 128],\n",
       "          [129, 130, 131]]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 2, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.transpose(1, 2).reshape(bsz * (q_len // group_size), group_size, num_heads, head_dim).transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = qkv.transpose(1, 2).reshape(bsz * (q_len // group_size), group_size, num_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0,   1,   2],\n",
       "          [  3,   4,   5]],\n",
       "\n",
       "         [[ 18,  19,  20],\n",
       "          [ 21,  22,  23]],\n",
       "\n",
       "         [[ 39,  40,  41],\n",
       "          [ 42,  43,  44]],\n",
       "\n",
       "         [[ 57,  58,  59],\n",
       "          [ 60,  61,  62]]],\n",
       "\n",
       "\n",
       "        [[[  6,   7,   8],\n",
       "          [  9,  10,  11]],\n",
       "\n",
       "         [[ 24,  25,  26],\n",
       "          [ 27,  28,  29]],\n",
       "\n",
       "         [[ 45,  46,  47],\n",
       "          [ 48,  49,  50]],\n",
       "\n",
       "         [[ 63,  64,  65],\n",
       "          [ 66,  67,  68]]],\n",
       "\n",
       "\n",
       "        [[[ 12,  13,  14],\n",
       "          [ 15,  16,  17]],\n",
       "\n",
       "         [[ 30,  31,  32],\n",
       "          [ 33,  34,  35]],\n",
       "\n",
       "         [[ 51,  52,  53],\n",
       "          [ 36,  37,  38]],\n",
       "\n",
       "         [[ 69,  70,  71],\n",
       "          [ 54,  55,  56]]],\n",
       "\n",
       "\n",
       "        [[[ 72,  73,  74],\n",
       "          [ 75,  76,  77]],\n",
       "\n",
       "         [[ 90,  91,  92],\n",
       "          [ 93,  94,  95]],\n",
       "\n",
       "         [[111, 112, 113],\n",
       "          [114, 115, 116]],\n",
       "\n",
       "         [[129, 130, 131],\n",
       "          [132, 133, 134]]],\n",
       "\n",
       "\n",
       "        [[[ 78,  79,  80],\n",
       "          [ 81,  82,  83]],\n",
       "\n",
       "         [[ 96,  97,  98],\n",
       "          [ 99, 100, 101]],\n",
       "\n",
       "         [[117, 118, 119],\n",
       "          [120, 121, 122]],\n",
       "\n",
       "         [[135, 136, 137],\n",
       "          [138, 139, 140]]],\n",
       "\n",
       "\n",
       "        [[[ 84,  85,  86],\n",
       "          [ 87,  88,  89]],\n",
       "\n",
       "         [[102, 103, 104],\n",
       "          [105, 106, 107]],\n",
       "\n",
       "         [[123, 124, 125],\n",
       "          [108, 109, 110]],\n",
       "\n",
       "         [[141, 142, 143],\n",
       "          [126, 127, 128]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.0000e+00],\n",
       "        ...,\n",
       "        [4.9970e+03],\n",
       "        [4.9980e+03],\n",
       "        [4.9990e+03]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 5000, dtype=torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New embeddings shape: torch.Size([2, 5, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 embeddings 的形状是 [2, 3, 4]\n",
    "embeddings = torch.randn(2, 3, 4)\n",
    "\n",
    "# 设置 batch_size 和 nums_head\n",
    "batch_size = 2\n",
    "nums_head = 5\n",
    "\n",
    "# 使用 repeat 扩展形状\n",
    "embeddings = embeddings.repeat((batch_size, nums_head, *([1] * len(embeddings.shape))))\n",
    "\n",
    "# 输出结果的形状\n",
    "print(\"New embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result shape: torch.Size([2, 3, 4, 2])\n",
      "Result tensor:\n",
      "tensor([[[[ 0.0000,  1.0000],\n",
      "          [ 0.8415,  0.5403],\n",
      "          [ 0.9093, -0.4161],\n",
      "          [ 0.1411, -0.9900]],\n",
      "\n",
      "         [[-0.7568, -0.6536],\n",
      "          [-0.9589,  0.2837],\n",
      "          [-0.2794,  0.9602],\n",
      "          [ 0.6570,  0.7539]],\n",
      "\n",
      "         [[ 0.9894, -0.1455],\n",
      "          [ 0.4121, -0.9111],\n",
      "          [-0.5440, -0.8391],\n",
      "          [-1.0000,  0.0044]]],\n",
      "\n",
      "\n",
      "        [[[-0.5366,  0.8439],\n",
      "          [ 0.4202,  0.9074],\n",
      "          [ 0.9906,  0.1367],\n",
      "          [ 0.6503, -0.7597]],\n",
      "\n",
      "         [[-0.2879, -0.9577],\n",
      "          [-0.9614, -0.2752],\n",
      "          [-0.7510,  0.6603],\n",
      "          [ 0.1499,  0.9887]],\n",
      "\n",
      "         [[ 0.9129,  0.4081],\n",
      "          [ 0.8367, -0.5477],\n",
      "          [-0.0089, -1.0000],\n",
      "          [-0.8462, -0.5328]]]])\n"
     ]
    }
   ],
   "source": [
    "def sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, device):\n",
    "\n",
    "max_position_embedding=2048\n",
    "base=10000\n",
    "device=None\n",
    "scaling_factor=1.0   \n",
    "# (max_len, 1)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(-1)\n",
    "\n",
    "# (output_dim//2)\n",
    "# 即公式里的i, i的范围是 [0,d/2]\n",
    "ids = torch.arange(0, output_dim // 2, dtype=torch.float)  \n",
    "theta = torch.pow(10000, -2 * ids / output_dim)\n",
    "\n",
    "# (max_len, output_dim//2)\n",
    "# 即公式里的：pos / (10000^(2i/d))\n",
    "embeddings = position * theta \n",
    "\n",
    "# (max_len, output_dim//2, 2)\n",
    "embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "\n",
    "# (bs, head, max_len, output_dim//2, 2)\n",
    "# 在bs维度重复，其他维度都是1不重复\n",
    "embeddings = embeddings.repeat((batch_size, nums_head, *([1] * len(embeddings.shape))))  \n",
    "\n",
    "# (bs, head, max_len, output_dim)\n",
    "# reshape后就是：偶数sin, 奇数cos了\n",
    "embeddings = torch.reshape(embeddings, (batch_size, nums_head, max_len, output_dim))\n",
    "embeddings = embeddings.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([[1, 2, 3]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个 1D 张量\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x.shape)  # torch.Size([3])\n",
    "\n",
    "# 在第 0 维增加一个维度\n",
    "x_unsqueeze = torch.unsqueeze(x, 0)\n",
    "print(x_unsqueeze)  # torch.Size([1, 3])\n",
    "\n",
    "# 在第 1 维增加一个维度\n",
    "x_unsqueeze = torch.unsqueeze(x, 1)\n",
    "print(x_unsqueeze)  # torch.Size([3, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusodial_position_embedding(batch_size, nums_head, max_len, output_dim, device):\n",
    "     \n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(-1)\n",
    " \n",
    "    # (output_dim//2)\n",
    "    # 即公式里的i, i的范围是 [0,d/2]\n",
    "    ids = torch.arange(0, output_dim//2, dtype=torch.float)  \n",
    "    theta = torch.pow(10000, -2 * ids / output_dim)\n",
    " \n",
    "    # (max_len, output_dim//2)\n",
    "    # 即公式里的：pos / (10000^(2i/d))\n",
    "    embeddings = position * theta \n",
    " \n",
    "    # (max_len, output_dim//2, 2)\n",
    "    embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    " \n",
    "    # (bs, head, max_len, output_dim//2, 2)\n",
    "    # 在bs维度重复，其他维度都是1不重复\n",
    "    embeddings = embeddings.repeat((batch_size, nums_head, *([1] * len(embeddings.shape))))  \n",
    " \n",
    "    # (bs, head, max_len, output_dim)\n",
    "    # reshape后就是：偶数sin, 奇数cos了\n",
    "    embeddings = torch.reshape(embeddings, (batch_size, nums_head, max_len, output_dim))\n",
    "    embeddings = embeddings.to(device)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.,  22.,\n",
       "         24.,  26.,  28.,  30.,  32.,  34.,  36.,  38.,  40.,  42.,  44.,  46.,\n",
       "         48.,  50.,  52.,  54.,  56.,  58.,  60.,  62.,  64.,  66.,  68.,  70.,\n",
       "         72.,  74.,  76.,  78.,  80.,  82.,  84.,  86.,  88.,  90.,  92.,  94.,\n",
       "         96.,  98., 100., 102., 104., 106., 108., 110., 112., 114., 116., 118.,\n",
       "        120., 122., 124., 126., 128., 130., 132., 134., 136., 138., 140., 142.,\n",
       "        144., 146., 148., 150., 152., 154., 156., 158., 160., 162., 164., 166.,\n",
       "        168., 170., 172., 174., 176., 178., 180., 182., 184., 186., 188., 190.,\n",
       "        192., 194., 196., 198., 200., 202., 204., 206., 208., 210., 212., 214.,\n",
       "        216., 218., 220., 222., 224., 226., 228., 230., 232., 234., 236., 238.,\n",
       "        240., 242., 244., 246., 248., 250., 252., 254., 256., 258., 260., 262.,\n",
       "        264., 266., 268., 270., 272., 274., 276., 278., 280., 282., 284., 286.,\n",
       "        288., 290., 292., 294., 296., 298., 300., 302., 304., 306., 308., 310.,\n",
       "        312., 314., 316., 318., 320., 322., 324., 326., 328., 330., 332., 334.,\n",
       "        336., 338., 340., 342., 344., 346., 348., 350., 352., 354., 356., 358.,\n",
       "        360., 362., 364., 366., 368., 370., 372., 374., 376., 378., 380., 382.,\n",
       "        384., 386., 388., 390., 392., 394., 396., 398., 400., 402., 404., 406.,\n",
       "        408., 410., 412., 414., 416., 418., 420., 422., 424., 426., 428., 430.,\n",
       "        432., 434., 436., 438., 440., 442., 444., 446., 448., 450., 452., 454.,\n",
       "        456., 458., 460., 462., 464., 466., 468., 470., 472., 474., 476., 478.,\n",
       "        480., 482., 484., 486., 488., 490., 492., 494., 496., 498., 500., 502.,\n",
       "        504., 506., 508., 510.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, output_dim, 2, dtype=torch.int64).float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0039, 0.0078, 0.0117, 0.0156, 0.0195, 0.0234, 0.0273, 0.0312,\n",
       "        0.0352, 0.0391, 0.0430, 0.0469, 0.0508, 0.0547, 0.0586, 0.0625, 0.0664,\n",
       "        0.0703, 0.0742, 0.0781, 0.0820, 0.0859, 0.0898, 0.0938, 0.0977, 0.1016,\n",
       "        0.1055, 0.1094, 0.1133, 0.1172, 0.1211, 0.1250, 0.1289, 0.1328, 0.1367,\n",
       "        0.1406, 0.1445, 0.1484, 0.1523, 0.1562, 0.1602, 0.1641, 0.1680, 0.1719,\n",
       "        0.1758, 0.1797, 0.1836, 0.1875, 0.1914, 0.1953, 0.1992, 0.2031, 0.2070,\n",
       "        0.2109, 0.2148, 0.2188, 0.2227, 0.2266, 0.2305, 0.2344, 0.2383, 0.2422,\n",
       "        0.2461, 0.2500, 0.2539, 0.2578, 0.2617, 0.2656, 0.2695, 0.2734, 0.2773,\n",
       "        0.2812, 0.2852, 0.2891, 0.2930, 0.2969, 0.3008, 0.3047, 0.3086, 0.3125,\n",
       "        0.3164, 0.3203, 0.3242, 0.3281, 0.3320, 0.3359, 0.3398, 0.3438, 0.3477,\n",
       "        0.3516, 0.3555, 0.3594, 0.3633, 0.3672, 0.3711, 0.3750, 0.3789, 0.3828,\n",
       "        0.3867, 0.3906, 0.3945, 0.3984, 0.4023, 0.4062, 0.4102, 0.4141, 0.4180,\n",
       "        0.4219, 0.4258, 0.4297, 0.4336, 0.4375, 0.4414, 0.4453, 0.4492, 0.4531,\n",
       "        0.4570, 0.4609, 0.4648, 0.4688, 0.4727, 0.4766, 0.4805, 0.4844, 0.4883,\n",
       "        0.4922, 0.4961, 0.5000, 0.5039, 0.5078, 0.5117, 0.5156, 0.5195, 0.5234,\n",
       "        0.5273, 0.5312, 0.5352, 0.5391, 0.5430, 0.5469, 0.5508, 0.5547, 0.5586,\n",
       "        0.5625, 0.5664, 0.5703, 0.5742, 0.5781, 0.5820, 0.5859, 0.5898, 0.5938,\n",
       "        0.5977, 0.6016, 0.6055, 0.6094, 0.6133, 0.6172, 0.6211, 0.6250, 0.6289,\n",
       "        0.6328, 0.6367, 0.6406, 0.6445, 0.6484, 0.6523, 0.6562, 0.6602, 0.6641,\n",
       "        0.6680, 0.6719, 0.6758, 0.6797, 0.6836, 0.6875, 0.6914, 0.6953, 0.6992,\n",
       "        0.7031, 0.7070, 0.7109, 0.7148, 0.7188, 0.7227, 0.7266, 0.7305, 0.7344,\n",
       "        0.7383, 0.7422, 0.7461, 0.7500, 0.7539, 0.7578, 0.7617, 0.7656, 0.7695,\n",
       "        0.7734, 0.7773, 0.7812, 0.7852, 0.7891, 0.7930, 0.7969, 0.8008, 0.8047,\n",
       "        0.8086, 0.8125, 0.8164, 0.8203, 0.8242, 0.8281, 0.8320, 0.8359, 0.8398,\n",
       "        0.8438, 0.8477, 0.8516, 0.8555, 0.8594, 0.8633, 0.8672, 0.8711, 0.8750,\n",
       "        0.8789, 0.8828, 0.8867, 0.8906, 0.8945, 0.8984, 0.9023, 0.9062, 0.9102,\n",
       "        0.9141, 0.9180, 0.9219, 0.9258, 0.9297, 0.9336, 0.9375, 0.9414, 0.9453,\n",
       "        0.9492, 0.9531, 0.9570, 0.9609, 0.9648, 0.9688, 0.9727, 0.9766, 0.9805,\n",
       "        0.9844, 0.9883, 0.9922, 0.9961])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.arange(0, output_dim, 2, dtype=torch.int64).float() / output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7737e-02,\n",
       "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7737e-03, 7.4989e-03, 7.2339e-03,\n",
       "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "        8.0584e-04, 7.7737e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "    self.max_seq_len_cached = seq_len\n",
    "    t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "    freqs = torch.outer(t, self.inv_freq)\n",
    "    # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "    self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "    self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 768, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PositionalEncoding(d_model=768, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，这只是一个简化的例子，真实的位置嵌入的值会有所不同。\n",
    "pos_emb = torch.tensor([[[[0.0000, 0.8415, 0.9093, 0.1411, 1.0000, 0.5403, -0.4161, -0.9900],\n",
    "                          [0.8415, 0.5403, 0.1411, -0.7568, 0.5403, -0.8415, -0.9900, -0.6536],\n",
    "                          [0.9093, -0.4161, -0.8415, -0.9589, -0.4161, -0.9093, -0.6536, 0.2836]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,  26,\n",
       "         28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,  54,\n",
       "         56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,  78,  80,  82,\n",
       "         84,  86,  88,  90,  92,  94,  96,  98, 100, 102, 104, 106, 108, 110,\n",
       "        112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138,\n",
       "        140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166,\n",
       "        168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194,\n",
       "        196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222,\n",
       "        224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250,\n",
       "        252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278,\n",
       "        280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306,\n",
       "        308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334,\n",
       "        336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362,\n",
       "        364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390,\n",
       "        392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418,\n",
       "        420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446,\n",
       "        448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474,\n",
       "        476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502,\n",
       "        504, 506, 508, 510])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 512, 2, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1.0 / (10000 ** (torch.arange(0, 512, 2, dtype=torch.int64).float()/512))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2., -4.],\n",
       "        [-6., -8.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-x[..., 1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 3.],\n",
       "        [5., 7.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[..., ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
    "                  [5.0, 6.0, 7.0, 8.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.,  1., -4.,  3.],\n",
       "        [-6.,  5., -8.,  7.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = torch.stack([-x[..., 1::2], x[..., ::2]], dim=-1)\n",
    "q2 = q2.reshape(x.shape) \n",
    "q2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.,   1., -12.,   9.],\n",
       "        [-30.,  25., -56.,  49.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin_pos * q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 2., 2., 3., 3., 4., 4.],\n",
       "        [5., 5., 6., 6., 7., 7., 8., 8.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sin_pos = x[..., :].repeat_interleave(2, dim=-1)\n",
    "sin_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., :x.shape[-1] // 2]  # 前半部分\n",
    "    x2 = x[..., x.shape[-1] // 2:]  # 后半部分\n",
    "    return torch.cat([-x2, x1], dim=-1)  # 交换并对后半部分取负\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3., -4.,  1.,  2.],\n",
      "        [-7., -8.,  5.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建示例张量\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
    "                  [5.0, 6.0, 7.0, 8.0]])\n",
    "\n",
    "# 调用 rotate_half 函数\n",
    "result = rotate_half(x)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_pos = pos_emb[..., ::2].repeat_interleave(2, dim=-1)  # 提取出所有sin编码，并在最后一个维度上复制\n",
    "cos_pos = pos_emb[..., 1::2].repeat_interleave(2, dim=-1)  # 提取出所有cos编码，并在最后一个维度上复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels: [3 1 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# 中文字符串数据\n",
    "data = [\"我喜欢编程\", \"我热爱编程\", \"编程让我快乐\", \"学习让我快乐\", \"我喜欢学习\"]\n",
    "\n",
    "# 将字符串转换为字符集合\n",
    "def string_to_set(s):\n",
    "    return set(s)\n",
    "\n",
    "# 计算两个字符串的Jaccard距离\n",
    "def jaccard_distance_set(s1, s2):\n",
    "    set1, set2 = string_to_set(s1), string_to_set(s2)\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return 1 - intersection / union if union != 0 else 1\n",
    "\n",
    "# 计算距离矩阵\n",
    "n = len(data)\n",
    "distance_matrix = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        distance = jaccard_distance_set(data[i], data[j])\n",
    "        distance_matrix[i][j] = distance\n",
    "        distance_matrix[j][i] = distance\n",
    "\n",
    "# 使用层次聚类\n",
    "clustering = AgglomerativeClustering(\n",
    "    distance_threshold=0.52,         # 设置距离阈值\n",
    "    n_clusters=None,                # 启用距离阈值\n",
    "    metric='precomputed',           # 使用自定义距离矩阵\n",
    "    linkage='average'               # 使用平均链接\n",
    ")\n",
    "labels = clustering.fit_predict(distance_matrix)\n",
    "\n",
    "print(\"Cluster labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_distance_set('我喜欢编程','我热爱编程')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1, set2 = string_to_set('学习让我快乐'), string_to_set('编程让我快乐')\n",
    "intersection = len(set1 & set2)\n",
    "union = len(set1 | set2)\n",
    "1 - intersection / union if union != 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'乐', '喜', '快', '我', '欢', '程', '编', '让'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 | set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters: [1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def custom_distance_matrix(data, labels=None):\n",
    "    \"\"\"\n",
    "    创建自定义的距离矩阵，结合余弦相似度和杰卡德距离。\n",
    "    \n",
    "    :param data: 数据点的特征矩阵 (n_samples, n_features)\n",
    "    :param labels: 如果数据是标签形式（集合），传入一个列表。\n",
    "    :return: 自定义距离矩阵\n",
    "    \"\"\"\n",
    "    if labels is not None:\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        data = mlb.fit_transform(labels)\n",
    "\n",
    "    # 余弦相似性转化为距离\n",
    "    cosine_sim = cosine_similarity(data)\n",
    "    cosine_dist = 1 - cosine_sim\n",
    "\n",
    "    # 计算杰卡德距离\n",
    "    def jaccard_distance(set_a, set_b):\n",
    "        intersection = np.logical_and(set_a, set_b).sum()\n",
    "        union = np.logical_or(set_a, set_b).sum()\n",
    "        return 1 - intersection / union if union != 0 else 1\n",
    "\n",
    "    jaccard_dist = np.zeros((len(data), len(data)))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            jaccard_dist[i, j] = jaccard_distance(data[i], data[j])\n",
    "\n",
    "    # 组合距离矩阵\n",
    "    combined_dist = 0.5 * cosine_dist + 0.5 * jaccard_dist\n",
    "\n",
    "    # 确保对角线为0\n",
    "    np.fill_diagonal(combined_dist, 0)\n",
    "    return combined_dist\n",
    "\n",
    "def hierarchical_clustering(data, labels=None, method='average', threshold=0.7):\n",
    "    \"\"\"\n",
    "    执行层次聚类，使用自定义距离矩阵。\n",
    "    \n",
    "    :param data: 数据点的特征矩阵 (n_samples, n_features)\n",
    "    :param labels: 如果数据是标签形式（集合），传入一个列表。\n",
    "    :param method: 层次聚类的方法，例如 'average', 'single', 'complete'\n",
    "    :param threshold: 距离阈值，确定聚类数量\n",
    "    :return: 聚类结果\n",
    "    \"\"\"\n",
    "    dist_matrix = custom_distance_matrix(data, labels)\n",
    "    # 压缩成下三角形式的距离矩阵\n",
    "    condensed_dist_matrix = squareform(dist_matrix, checks=False)\n",
    "    linkage_matrix = linkage(condensed_dist_matrix, method=method)\n",
    "    clusters = fcluster(linkage_matrix, t=threshold, criterion='distance')\n",
    "    return clusters, linkage_matrix\n",
    "\n",
    "# 示例数据\n",
    "data = np.array([[1, 0, 1, 0], [1, 1, 1, 0], [0, 1, 0, 1], [0, 0, 1, 1]])\n",
    "labels = [['apple', 'banana'], ['apple'], ['banana', 'orange'], ['orange']]\n",
    "\n",
    "# 聚类\n",
    "clusters, linkage_matrix = hierarchical_clustering(data, labels)\n",
    "\n",
    "# 可视化树状图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linkage_matrix, labels=[f\"Data {i}\" for i in range(len(data))])\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Data Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Clusters:\", clusters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
